{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# import used library\n","import kagglehub\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import RobustScaler\n","from google.colab import drive\n","import shutil"],"metadata":{"id":"a-Y221HOjYHT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define constant values\n","# calculate sampling step\n","final_sample_amount = 15000\n","sampling_step = 30420086 // final_sample_amount"],"metadata":{"id":"I9xl-VscjtZ_"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"awnprI1SIQnp","outputId":"7641b77d-d7d7-4945-d1d9-9ab2c8b68439","executionInfo":{"status":"ok","timestamp":1732847788110,"user_tz":480,"elapsed":20509,"user":{"displayName":"Saman Attar Kashani","userId":"10851118940244856964"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading from https://www.kaggle.com/api/v1/datasets/download/dhoogla/nfbotiotv2?dataset_version_number=2...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 421M/421M [00:07<00:00, 58.5MB/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting files...\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Path to dataset files: /root/.cache/kagglehub/datasets/dhoogla/nfbotiotv2/versions/2\n"]}],"source":["# Download latest version\n","path = kagglehub.dataset_download(\"dhoogla/nfbotiotv2\")\n","\n","print(\"Path to dataset files:\", path)"]},{"cell_type":"code","source":["# Define the path to the Parquet file\n","parquet_file_path = f\"{path}/NF-BoT-IoT-V2.parquet\"\n","\n","# Load the Parquet file into a pandas DataFrame\n","full_df = pd.read_parquet(parquet_file_path)\n","\n","# Replace inf and -inf with NaN\n","full_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n","\n","# Drop rows with any NaN values\n","full_df.dropna(inplace=True)"],"metadata":{"id":"1Lk6jcCn42ZX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Shape of primary data set is {full_df.shape}\")\n","print(f\"The number of samples in main data set is equal to: {len(full_df)}\")"],"metadata":{"id":"EHGVsbBGJth4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"826f8edf-d1c2-4e6e-f5a6-b09eb8f65e9d","executionInfo":{"status":"ok","timestamp":1732847818138,"user_tz":480,"elapsed":21,"user":{"displayName":"Saman Attar Kashani","userId":"10851118940244856964"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of primary data set is (30420086, 43)\n","The number of samples in main data set is equal to: 30420086\n"]}]},{"cell_type":"code","source":["# Assuming full_df is your existing DataFrame\n","# Create a new DataFrame with rows at multiple indices of sampling_step\n","shorter_df = full_df.iloc[range(0, len(full_df), sampling_step)]\n","\n","# Save the new DataFrame to a CSV file\n","shorter_df.to_csv('shorter dataset.csv', index=False)"],"metadata":{"id":"j8hxkN1P2Iy8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Shape of secondary data set is {shorter_df.shape}\")\n","print(f\"The number of samples in shortened data set is equal to: {len(shorter_df)}\")"],"metadata":{"id":"RYjOk-tzLhD5","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7f047581-56e9-4432-aef7-ead0423c63c1","executionInfo":{"status":"ok","timestamp":1732847818139,"user_tz":480,"elapsed":13,"user":{"displayName":"Saman Attar Kashani","userId":"10851118940244856964"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of secondary data set is (15001, 43)\n","The number of samples in shortened data set is equal to: 15001\n"]}]},{"cell_type":"code","source":["# Assuming shorter_df is your DataFrame\n","# Get the last column name (assuming it's the label column)\n","label_column = shorter_df.columns[-1]\n","\n","# Count occurrences of each label\n","label_counts = shorter_df[label_column].value_counts()\n","\n","# Calculate percentages\n","label_percentages = (label_counts / len(shorter_df)) * 100\n","\n","# Create a summary DataFrame\n","summary_df = pd.DataFrame({\n","    'Count': label_counts,\n","    'Percentage': label_percentages\n","})\n","\n","# Print the summary\n","print(summary_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6W1wHKKVmjpB","outputId":"ae2444f4-4b2e-4008-abe4-52560817a7a9","executionInfo":{"status":"ok","timestamp":1732847818708,"user_tz":480,"elapsed":580,"user":{"displayName":"Saman Attar Kashani","userId":"10851118940244856964"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                Count  Percentage\n","Attack                           \n","DDoS             7070   47.130191\n","DoS              6719   44.790347\n","Reconnaissance   1139    7.592827\n","Benign             72    0.479968\n","Theft               1    0.006666\n"]}]},{"cell_type":"code","source":["# Print one row of the dataset\n","print(shorter_df.iloc[658])  # Prints example row of the DataFrame"],"metadata":{"id":"CO0ODBwq7817","colab":{"base_uri":"https://localhost:8080/"},"outputId":"180a76b5-f479-40a2-d9cd-40ef01cbabae","executionInfo":{"status":"ok","timestamp":1732847818708,"user_tz":480,"elapsed":21,"user":{"displayName":"Saman Attar Kashani","userId":"10851118940244856964"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["L4_SRC_PORT                      10504\n","L4_DST_PORT                         80\n","PROTOCOL                            17\n","L7_PROTO                         188.0\n","IN_BYTES                            56\n","IN_PKTS                              2\n","OUT_BYTES                            0\n","OUT_PKTS                             0\n","TCP_FLAGS                            0\n","CLIENT_TCP_FLAGS                     0\n","SERVER_TCP_FLAGS                     0\n","FLOW_DURATION_MILLISECONDS     4293951\n","DURATION_IN                       1016\n","DURATION_OUT                         0\n","MIN_TTL                             64\n","MAX_TTL                             64\n","LONGEST_FLOW_PKT                    28\n","SHORTEST_FLOW_PKT                   28\n","MIN_IP_PKT_LEN                       0\n","MAX_IP_PKT_LEN                      28\n","SRC_TO_DST_SECOND_BYTES         2828.0\n","DST_TO_SRC_SECOND_BYTES            0.0\n","RETRANSMITTED_IN_BYTES               0\n","RETRANSMITTED_IN_PKTS                0\n","RETRANSMITTED_OUT_BYTES              0\n","RETRANSMITTED_OUT_PKTS               0\n","SRC_TO_DST_AVG_THROUGHPUT       224000\n","DST_TO_SRC_AVG_THROUGHPUT            0\n","NUM_PKTS_UP_TO_128_BYTES             2\n","NUM_PKTS_128_TO_256_BYTES            0\n","NUM_PKTS_256_TO_512_BYTES            0\n","NUM_PKTS_512_TO_1024_BYTES           0\n","NUM_PKTS_1024_TO_1514_BYTES          0\n","TCP_WIN_MAX_IN                       0\n","TCP_WIN_MAX_OUT                      0\n","ICMP_TYPE                            0\n","ICMP_IPV4_TYPE                       0\n","DNS_QUERY_ID                         0\n","DNS_QUERY_TYPE                       0\n","DNS_TTL_ANSWER                       0\n","FTP_COMMAND_RET_CODE               0.0\n","Label                                1\n","Attack                            DDoS\n","Name: 1334424, dtype: object\n"]}]},{"cell_type":"code","source":["# Assuming shorter_df is your DataFrame\n","# Split the DataFrame into training and testing sets\n","train_df, test_df = train_test_split(shorter_df, test_size=0.2, random_state=42)\n","\n","# Function to calculate label percentages\n","def label_percentage(df):\n","    label_column = df.columns[-1]  # Assuming the last column is the label column\n","    label_counts = df[label_column].value_counts()\n","    percentages = (label_counts / len(df)) * 100\n","    return pd.DataFrame({'Count': label_counts, 'Percentage': percentages})\n","\n","# Calculate percentages for training and testing sets\n","train_summary = label_percentage(train_df)\n","test_summary = label_percentage(test_df)\n","\n","# Print the summaries\n","print(\"Training Set Label Summary:\")\n","print(train_summary)\n","\n","print(\"\\nTesting Set Label Summary:\")\n","print(test_summary)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o8Hn8NSloudq","outputId":"566b7a05-6083-484e-fe42-00f4de15a893","executionInfo":{"status":"ok","timestamp":1732847818709,"user_tz":480,"elapsed":18,"user":{"displayName":"Saman Attar Kashani","userId":"10851118940244856964"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Set Label Summary:\n","                Count  Percentage\n","Attack                           \n","DDoS             5671   47.258333\n","DoS              5340   44.500000\n","Reconnaissance    934    7.783333\n","Benign             54    0.450000\n","Theft               1    0.008333\n","\n","Testing Set Label Summary:\n","                Count  Percentage\n","Attack                           \n","DDoS             1399   46.617794\n","DoS              1379   45.951350\n","Reconnaissance    205    6.831056\n","Benign             18    0.599800\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.preprocessing import MinMaxScaler\n","\n","def min_max_normalize(train_df, test_df):\n","    \"\"\"\n","    Normalize the training and testing datasets using Min-Max normalization.\n","\n","    Parameters:\n","    - train_df: DataFrame containing the training data.\n","    - test_df: DataFrame containing the testing data.\n","\n","    Returns:\n","    - normalized_train_df: DataFrame with normalized training data.\n","    - normalized_test_df: DataFrame with normalized testing data.\n","    \"\"\"\n","\n","    # Create a MinMaxScaler object\n","    scaler = MinMaxScaler()\n","\n","    # Separate features from labels in the training set\n","    train_features = train_df.iloc[:, :-2]  # All columns except the last two\n","    train_labels = train_df.iloc[:, -2:]     # Last two columns (labels)\n","\n","    # Fit the scaler on the training features and transform them\n","    normalized_train_features = scaler.fit_transform(train_features)\n","\n","    # Create a new DataFrame for normalized training data\n","    normalized_train_df = pd.DataFrame(normalized_train_features, columns=train_features.columns)\n","\n","    # Add back the labels to the normalized training DataFrame\n","    normalized_train_df = pd.concat([normalized_train_df, train_labels.reset_index(drop=True)], axis=1)\n","\n","    # Separate features from labels in the testing set\n","    test_features = test_df.iloc[:, :-2]  # All columns except the last two\n","    test_labels = test_df.iloc[:, -2:]     # Last two columns (labels)\n","\n","    # Transform the testing features using the same scaler\n","    normalized_test_features = scaler.transform(test_features)\n","\n","    # Create a new DataFrame for normalized testing data\n","    normalized_test_df = pd.DataFrame(normalized_test_features, columns=test_features.columns)\n","\n","    # Add back the labels to the normalized testing DataFrame\n","    normalized_test_df = pd.concat([normalized_test_df, test_labels.reset_index(drop=True)], axis=1)\n","\n","    return normalized_train_df, normalized_test_df\n","\n","# Example usage:\n","# Assuming you have your train and test DataFrames ready as train_df and test_df\n","# normalized_train, normalized_test = normalize_datasets(train_df, test_df)\n","\n","# Normalize both training and testing sets\n","train_normalized, test_normalized = min_max_normalize(train_df, test_df)"],"metadata":{"id":"x0iPP9w-ovHS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save normalized training and testing sets to CSV files\n","train_normalized.to_csv('normalized_train.csv', index=False)\n","test_normalized.to_csv('normalized_test.csv', index=False)\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Move the CSV files to Google Drive\n","drive_path = '/content/drive/MyDrive/IDS Dataset 2/'\n","shutil.move('normalized_train.csv', drive_path + 'NF-BoT-IoT-V2_' + str(final_sample_amount) + ' samples_minmax_normalized_train.csv')\n","shutil.move('normalized_test.csv', drive_path + 'NF-BoT-IoT-V2_' + str(final_sample_amount) + ' samples_minmax_normalized_test.csv')\n","\n","print(\"Files have been uploaded to Google Drive.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z5oSufassB9d","outputId":"e580d4b3-af9d-447c-8623-f6899d3e0b3e","executionInfo":{"status":"ok","timestamp":1732847973799,"user_tz":480,"elapsed":4305,"user":{"displayName":"Saman Attar Kashani","userId":"10851118940244856964"}}},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Files have been uploaded to Google Drive.\n"]}]},{"cell_type":"code","source":["# Print one row of the dataset\n","print(train_normalized.iloc[658])  # Prints example row of the DataFrame"],"metadata":{"id":"OVH6J9osMYgf","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a6930fc8-baea-46a0-eeab-af6bc80345d2","executionInfo":{"status":"ok","timestamp":1732847975929,"user_tz":480,"elapsed":346,"user":{"displayName":"Saman Attar Kashani","userId":"10851118940244856964"}}},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["L4_SRC_PORT                     0.32815\n","L4_DST_PORT                    0.501252\n","PROTOCOL                            1.0\n","L7_PROTO                       0.789916\n","IN_BYTES                       0.002941\n","IN_PKTS                        0.021739\n","OUT_BYTES                           0.0\n","OUT_PKTS                            0.0\n","TCP_FLAGS                           0.0\n","CLIENT_TCP_FLAGS                    0.0\n","SERVER_TCP_FLAGS                    0.0\n","FLOW_DURATION_MILLISECONDS     0.999746\n","DURATION_IN                    0.440241\n","DURATION_OUT                        0.0\n","MIN_TTL                             1.0\n","MAX_TTL                             1.0\n","LONGEST_FLOW_PKT                    0.0\n","SHORTEST_FLOW_PKT                   0.0\n","MIN_IP_PKT_LEN                      0.0\n","MAX_IP_PKT_LEN                      0.0\n","SRC_TO_DST_SECOND_BYTES         0.00001\n","DST_TO_SRC_SECOND_BYTES             0.0\n","RETRANSMITTED_IN_BYTES              0.0\n","RETRANSMITTED_IN_PKTS               0.0\n","RETRANSMITTED_OUT_BYTES             0.0\n","RETRANSMITTED_OUT_PKTS              0.0\n","SRC_TO_DST_AVG_THROUGHPUT      0.002933\n","DST_TO_SRC_AVG_THROUGHPUT           0.0\n","NUM_PKTS_UP_TO_128_BYTES       0.038462\n","NUM_PKTS_128_TO_256_BYTES           0.0\n","NUM_PKTS_256_TO_512_BYTES           0.0\n","NUM_PKTS_512_TO_1024_BYTES          0.0\n","NUM_PKTS_1024_TO_1514_BYTES         0.0\n","TCP_WIN_MAX_IN                      0.0\n","TCP_WIN_MAX_OUT                     0.0\n","ICMP_TYPE                           0.0\n","ICMP_IPV4_TYPE                      0.0\n","DNS_QUERY_ID                        0.0\n","DNS_QUERY_TYPE                      0.0\n","DNS_TTL_ANSWER                      0.0\n","FTP_COMMAND_RET_CODE                0.0\n","Label                                 1\n","Attack                             DDoS\n","Name: 658, dtype: object\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Dt0qcOzkybmM"},"execution_count":null,"outputs":[]}]}